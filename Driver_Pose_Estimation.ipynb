{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete dataset path\n",
    "full_data_dir = 'C:\\\\Users\\\\SAHIL\\\\Driver-Pose-Dataset\\\\imgs\\\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a path for base directory to work on\n",
    "base_dir = './IMAGES'\n",
    "pathlib.Path(base_dir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation and test data to split and work\n",
    "train_dir = os.path.join(base_dir, 'Train')\n",
    "pathlib.Path(train_dir).mkdir(exist_ok=True)\n",
    "\n",
    "validation_dir = os.path.join(base_dir, 'Validation')\n",
    "pathlib.Path(validation_dir).mkdir(exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(base_dir, 'Test')\n",
    "pathlib.Path(test_dir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Train data\n",
      "Copying c0 data\n",
      "Copying c1 data\n",
      "Copying c2 data\n",
      "Copying c3 data\n",
      "Copying c4 data\n",
      "Copying c5 data\n",
      "Copying c6 data\n",
      "Copying c7 data\n",
      "Copying c8 data\n",
      "Copying c9 data\n",
      "Copying Validation data\n",
      "Copying c0 data\n",
      "Copying c1 data\n",
      "Copying c2 data\n",
      "Copying c3 data\n",
      "Copying c4 data\n",
      "Copying c5 data\n",
      "Copying c6 data\n",
      "Copying c7 data\n",
      "Copying c8 data\n",
      "Copying c9 data\n",
      "Copying Test data\n",
      "Copying c0 data\n",
      "Copying c1 data\n",
      "Copying c2 data\n",
      "Copying c3 data\n",
      "Copying c4 data\n",
      "Copying c5 data\n",
      "Copying c6 data\n",
      "Copying c7 data\n",
      "Copying c8 data\n",
      "Copying c9 data\n"
     ]
    }
   ],
   "source": [
    "#Copying images in the required fashion using manual splitting\n",
    "dirs = {'Train': train_dir, 'Validation': validation_dir, 'Test': test_dir}\n",
    "\n",
    "classnames = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n",
    "\n",
    "for (dataset_type, start, end) in [('Train', 0, 1300), ('Validation', 1300, 1600), ('Test', 1600, 1900)]:\n",
    "    print('Copying ' + dataset_type + ' data')\n",
    "    for classname in classnames:\n",
    "        print('Copying ' + classname + ' data')\n",
    "        pathlib.Path(os.path.join(dirs[dataset_type], classname)).mkdir(exist_ok=True)\n",
    "        file_names = os.listdir(os.path.join(full_data_dir, classname))[start:end]\n",
    "        for file_name in file_names:\n",
    "            src = os.path.join(full_data_dir, classname, file_name)\n",
    "            dst = os.path.join(dirs[dataset_type], classname, file_name)\n",
    "            shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential model - CNN\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(230, 230, 3)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13000 images belonging to 10 classes.\n",
      "Found 3000 images belonging to 10 classes.\n",
      "WARNING:tensorflow:From <ipython-input-9-b6facd676ade>:21: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 260 steps, validate for 60 steps\n",
      "Epoch 1/9\n",
      "260/260 [==============================] - 622s 2s/step - loss: 1.2866 - acc: 0.5426 - val_loss: 0.4360 - val_acc: 0.8743\n",
      "Epoch 2/9\n",
      "260/260 [==============================] - 660s 3s/step - loss: 0.3492 - acc: 0.8929 - val_loss: 0.1885 - val_acc: 0.9443\n",
      "Epoch 3/9\n",
      "260/260 [==============================] - 1763s 7s/step - loss: 0.1847 - acc: 0.9427 - val_loss: 0.1032 - val_acc: 0.9700\n",
      "Epoch 4/9\n",
      "260/260 [==============================] - 637s 2s/step - loss: 0.1326 - acc: 0.9594 - val_loss: 0.0775 - val_acc: 0.9793\n",
      "Epoch 5/9\n",
      "260/260 [==============================] - 631s 2s/step - loss: 0.0897 - acc: 0.9717 - val_loss: 0.0777 - val_acc: 0.9807\n",
      "Epoch 6/9\n",
      "260/260 [==============================] - 622s 2s/step - loss: 0.0736 - acc: 0.9766 - val_loss: 0.0736 - val_acc: 0.9807\n",
      "Epoch 7/9\n",
      "260/260 [==============================] - 620s 2s/step - loss: 0.0644 - acc: 0.9791 - val_loss: 0.0606 - val_acc: 0.9847\n",
      "Epoch 8/9\n",
      "260/260 [==============================] - 634s 2s/step - loss: 0.0471 - acc: 0.9846 - val_loss: 0.0525 - val_acc: 0.9877\n",
      "Epoch 9/9\n",
      "260/260 [==============================] - 629s 2s/step - loss: 0.0473 - acc: 0.9844 - val_loss: 0.0699 - val_acc: 0.9820\n"
     ]
    }
   ],
   "source": [
    "#generators for our model\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # All images will be resized to 230x230\n",
    "        target_size=(230, 230),\n",
    "        batch_size=50,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(230, 230),\n",
    "        batch_size=50,\n",
    "        class_mode='categorical')\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=260,\n",
    "      epochs=9,\n",
    "      verbose = 1,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "60/60 [==============================] - 35s 585ms/step - loss: 0.0587 - acc: 0.9883\n",
      "test_loss acc: 0.05866525627983113\n",
      "test acc: 0.98833334\n"
     ]
    }
   ],
   "source": [
    "#evaluating on test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(230, 230),\n",
    "        batch_size=50,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=60)\n",
    "print('test_loss acc:', test_loss)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class labels\n",
    "class_labels = [\n",
    "    \"Normal Driving\",\n",
    "    \"Texting - Right Hand\",\n",
    "    \"Talking on the phone - Right Hand\",\n",
    "    \"Texting - Left Hand\",\n",
    "    \"Talking on the phone - Left Hand\",\n",
    "    \"Operating the Radio\",\n",
    "    \"Drinking\",\n",
    "    \"Reaching behind\",\n",
    "    \"Hair and makeup\",\n",
    "    \"Talking to passenger\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#testing on an image manually selected by the user\n",
    "test_image_path = \"./img_100038.jpg\"\n",
    "\n",
    "img_array = []\n",
    "\n",
    "img = image.load_img(test_image_path, target_size=(230, 230))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.array([img_array])\n",
    "\n",
    "img_array = img_array.astype('float32') / 255\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "label_index = int(np.argmax(predictions, axis = 1))\n",
    "print(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the output using openCV\n",
    "test_image = cv2.imread(test_image_path)\n",
    "\n",
    "cv2.putText(test_image, class_labels[label_index], (320, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    \n",
    "cv2.imshow(\"Image\",test_image)\n",
    "    \n",
    "cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
